{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domain Background "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a world where advertising is becoming increasingly competetive, it is becoming also more and more important to target your audience accurately in order to ensure your business can continue to thrive.\n",
    "With the recent advancements in computing, machine learning algorithms are now largely available in a number of frameworks that makes it easy for everybody to train and use. Leveraging this new technology to target your customers accurately is nowadays paramount.\n",
    "Starbucks is a coffee company that was founded in 1971 in Seattle and despite his strong presence is the market, the recent pandemic hit the sector quite strongly. Starbucks reported his first quarterly loss in 7 years (source: https://www.ft.com/content/4c876c5a-a03a-3f57-8a04-c364b12ad4b7) and many bars and restaurants had to close down as their profitability was strongly undermined by the social distancing guidelines put in place by world governments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starbucks wants to improve the way it proposes deals and offers through the app to its customers. The aim of this project is to leverage machine learning algorithms to increase revenue by targeting the right audience and drawing them to the stores. The goal of this project is to analyse historical data to draw insights and create a model to better target customers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets and Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasets for this projects have been supplied by Starbucks. This data mimics customer behaviour.\n",
    "\n",
    "The datasets include the following:\n",
    "17000 customers and some demographic data (age, income, when they became customers)\n",
    "306534 events between transactions of purchases and offers\n",
    "and some more information on the offers. \n",
    "We have BOGO offers, informational and discounts. \n",
    "We also know the channels used to extend these offers (email, web, mobile,social).\n",
    "\n",
    "Another key information I have extracted is the type of event in our dataset and they are classified as follows:\n",
    "\n",
    "transaction        138953\n",
    "offer received      76277\n",
    "offer viewed        57725\n",
    "offer completed     33579\n",
    "\n",
    "And since we are interested in how many of the offers sent have been completed, we know that approximately 59% of the offers were completed ((76277-33579)/76277)*100.\n",
    "\n",
    "This means that the dataset we have at hands is also fairly balanced and we can use accuracy to measure the success of our models. \n",
    "\n",
    "The first step will be to combine the 3 datasets and then split the data into training, validation and test set using a 70/15/15 split. The splits will be stratified using the target variable to make sure that each sample is balanced. \n",
    "\n",
    "Please find below more information on the 3 datasets at hand:\n",
    "\n",
    "* portfolio.json - containing offer ids and meta data about each offer (duration, type, etc.)\n",
    "* profile.json - demographic data for each customer\n",
    "* transcript.json - records for transactions, offers received, offers viewed, and offers completed\n",
    "\n",
    "\n",
    "Here is the schema and explanation of each variable in the files:\n",
    "\n",
    "portfolio.json\n",
    "\n",
    "* id (string) - offer id\n",
    "* offer_type (string) - type of offer ie BOGO, discount, informational\n",
    "* difficulty (int) - minimum required spend to complete an offer\n",
    "* reward (int) - reward given for completing an offer\n",
    "* duration (int) - time for offer to be open, in days\n",
    "* channels (list of strings)\n",
    "\n",
    "profile.json\n",
    "\n",
    "* age (int) - age of the customer\n",
    "* became_member_on (int) - date when customer created an app account\n",
    "* gender (str) - gender of the customer (note some entries contain 'O' for other rather than M or F)\n",
    "* id (str) - customer id\n",
    "* income (float) - customer's income\n",
    "\n",
    "transcript.json\n",
    "\n",
    "* event (str) - record description (ie transaction, offer received, offer viewed, etc.)\n",
    "* person (str) - customer id\n",
    "* time (int) - time in hours since start of test. The data begins at time t=0\n",
    "* value - (dict of strings) - either an offer id or transaction amount depending on the record\n",
    "\n",
    "We will leverage historical patterns on which customers accepted offers received through the app, and use that data to build ML models to predict whether a customer is likely to complete the offer upon receipt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to build a Supervised Machine Learning model to predict whether the customer will complete the offer or not based on historical patterns. The success will be measured based on accuracy (Offers completed/Offers sent).\n",
    "The solution will be a supervised Learning Model, the best performing among the following: XGBoost, Random Forest and Logistic Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In binary classification tasks Logistic Regression is widely used, and we will also use it here as benchmark to compare other machine learning algorithms against.\n",
    "We will track progress agains the results achieved by Logistic Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project will follow the classic Machine Learning Workflow and I will carry out the following steps/analyses:\n",
    "\n",
    "1) EDA: Exploratory Data Analysis: In this stem we will analyse the distribution and correlations among the variables in the           dataset. Exploring data may include histograms and visualizations of the classes at hand (gender, income) and                   relationship that each feature has with the target variable (boxplots) to get a feeling for which variables will be             more informing.\n",
    "\n",
    "2) Data pre-processing: We currently have 3 datasets and classic machine learning algorithms work with a single Matrix \n",
    "                                 or tabular data that contains the features and a target variable for our prediction.\n",
    "                        First step is to merge the 3 datasets leveraging the built in functions in pandas.\n",
    "                        We will then need to remove duplicates and/or missing values if any and remove the transactions not related to offers. Further data cleaning steps will be added as we start analysing the datasets more deeply.\n",
    "                        \n",
    "\n",
    "3) Feature Engineering/Selection: In this step we will create variables from the current datasets we have that we will then use                                   in the final model. We will also select the most relevant variables for our prediction and                                     retain those with the highest explanatory power. What features can be transformed and added will become clearer as i explore the data more but one example will be to turn the start of the membership date into days since the customer joined (tenure). Scaling the data will also be included using the MinMax Scaler or standard scaler of Scikit learn library.\n",
    "\n",
    "4) Data Modelling: In this step we will fit a number of machine learning algorithms. We will leverage scikit learn in python to fit the Logistic regression model which will be our benchmark. We will then fit XGBoost and Random Forest and benchmark the results against Logistic regression. We will use Random Search to fine tune the hyperparameters of XGBoost and Random Forest, scikit learn offers some built in function to do so. Due to the many hyperparameters to tune and the high number of combinations among them, we have chosen Random Search to save computational time.\n",
    "\n",
    "The results of the analyses will be compiled in a separate report along with the recommendations on which algorithm to use."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
